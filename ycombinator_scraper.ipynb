{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **INFORMATION TO BE SCRAPED**\n",
    "\n",
    "The image below indicates the information to be scraped from the website: https://www.ycombinator.com/companies\n",
    "\n",
    "\n",
    "<img width=\"\" alt=\"Screenshot 2022-04-03 at 7 29 58 PM\" src=\"https://user-images.githubusercontent.com/55639062/161443204-ae7fc423-f1d3-4512-bb56-7bef85f3691e.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEPS\n",
    "1) Launch the chrome webdriver and open the link on the webdriver.\n",
    "2) Use the selenium library to scroll to the end of the page.\n",
    "3) Extract the links (href) to the full information on each company and pass it into beautifulsoup.\n",
    "4) Use the beautifulsoup library to scrape all of the necessary information on each company.\n",
    "5) Store the scraped information in a pandas dataframe and export to a suitable format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRAS\n",
    "1) Resilience to network failure.\n",
    "2) Multi-threading for faster runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from IPython.display import Image\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import socket\n",
    "import concurrent.futures as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION TO CHECK THE INTERNET CONNECTIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def internet_on():\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlopen('https://www.google.com', timeout=2)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION TO SETUP CHROME BROWSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_setup():\n",
    "    \n",
    "    # Path to the Chromedriver.\n",
    "    PATH = \"C:/Users/famou/.wdm/drivers/chromedriver/win32/99.0.4844.51/chromedriver.exe\"\n",
    "\n",
    "    # Change options.headless to False to show chrome browser.\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(PATH, chrome_options=options)\n",
    "\n",
    "    # Load url in browser.\n",
    "    URL = 'https://www.ycombinator.com/companies/'\n",
    "    driver.get(URL)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION TO CONFIGURE SELENIUM FOR INFINITE SCROLLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_scroll(driver):\n",
    "    \n",
    "    try:\n",
    "        # Wait for some elements to load.\n",
    "        WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, \n",
    "                        \"/html/body/div/div[2]/div/div/div[2]/div[4]/a[1]/div[1]/div/img\")))\n",
    "        \n",
    "        \n",
    "        SCROLL_PAUSE_TIME = 0.5\n",
    "        \n",
    "        # Get scroll height\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Loading took too much time!!. Please refresh.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION TO GET THE URL AND LOCATION OF ALL COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_url_and_country():\n",
    "\n",
    "    homepage = browser_setup()\n",
    "    infinite_scroll(homepage)\n",
    "    \n",
    "    page = homepage.page_source\n",
    "    ycombinator = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    # Find all the anchor tags with class = \"styles-module__company___1UVnl no-hovercard\"\n",
    "    # and return their \"href\".\n",
    "    sites = []\n",
    "    count = 0\n",
    "    for link in ycombinator.find_all('a',\n",
    "                            class_ = \"styles-module__company___1UVnl no-hovercard\"):\n",
    "        # Get the actual urls.\n",
    "        site = link.get('href') \n",
    "        site_mod = f\"https://www.ycombinator.com{site}\"\n",
    "        sites.append(site_mod)\n",
    "        count += 1\n",
    "\n",
    "        # Extract location.\n",
    "        hold = link.find('span', class_='styles-module__coLocation___yhKam')\n",
    "        Location.append(hold.text)\n",
    "\n",
    "        # Extract the country of each company.\n",
    "        country = str(hold.text).split(\",\")[-1].strip()\n",
    "        Country.append(country)\n",
    "\n",
    "    # Count number of links.\n",
    "    print(f\"\\nCompanies total:\", count)\n",
    "    print('\\n')\n",
    "    return sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION TO SCRAPE COMPANIES DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies_info(soup):\n",
    "    \n",
    "    # Company name.\n",
    "    select = soup.find('div', class_=\"font-bold text-lg\")\n",
    "    Company_name.append(select.text)\n",
    "\n",
    "    # Company tag.\n",
    "    hold = []\n",
    "    select = soup.find_all('span',  class_='ycdc-badge')\n",
    "    for j in select:\n",
    "        j = j.text.replace('Y Combinator Logo', '')\n",
    "        hold.append(j)\n",
    "\n",
    "    Company_tag.append(hold)    \n",
    "\n",
    "    # Short description.\n",
    "    select = soup.find('h3', class_=\"sm:block md:hidden\")\n",
    "    Short_description.append(select.text)\n",
    "\n",
    "    # Founded.\n",
    "    select = soup.find('div', class_ = \"space-y-0.5\")\n",
    "    select = select.next_element\n",
    "    for j in select:\n",
    "        if j.text != 'Founded:':\n",
    "            Founded.append(j.text)\n",
    "\n",
    "    # Team Size.\n",
    "    select = select.next_sibling\n",
    "    for j in select:\n",
    "        if j.text != 'Team Size:':\n",
    "            Team_size.append(j.text)\n",
    "\n",
    "    # Location.\n",
    "    # Check the function get_company_url_and_country.\n",
    "    \n",
    "    # Website.\n",
    "    for j in soup.find('div', class_ = \"flex flex-row items-center leading-none px-3\"):\n",
    "        if j.get('href') != None:\n",
    "            Website.append(j.get('href'))\n",
    "\n",
    "    # Company's social media links.\n",
    "    hold = []\n",
    "    for j in soup.find(class_ = \"space-x-2\"):\n",
    "        link = j.get('href')\n",
    "        hold.append(link)\n",
    "\n",
    "    Social_media_company.append(hold)\n",
    "\n",
    "    # Full description.\n",
    "    select = soup.find('p', class_=\"whitespace-pre-line\")\n",
    "    Description.append(select.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION TO SCRAPE FOUNDERS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_founders_info(soup):\n",
    "\n",
    "    try:\n",
    "        # Active founders.\n",
    "        hold = []\n",
    "        founders = soup.find('div', class_='space-y-5')\n",
    "        for name in founders.find_all('div', class_='leading-snug'):\n",
    "            hold.append(name.div.text)\n",
    "        Active_founders.append(hold)\n",
    "\n",
    "        # Founders details.\n",
    "        all_info = []\n",
    "        for what in founders.find_all('div', class_='flex flex-row gap-3 items-start flex-col md:flex-row'):\n",
    "            info = {}\n",
    "            name = what.h3.text\n",
    "            split = name.split(', ')\n",
    "\n",
    "            info['name'] = split[0]\n",
    "\n",
    "            if split[0] != split[-1]:\n",
    "                info['role'] = split[-1]\n",
    "            else:\n",
    "                info['role'] = ''\n",
    "\n",
    "            \n",
    "            info['social_media'] = [link['href'] for link in what.find('div', class_='mt-1 space-x-2').find_all('a')]\n",
    "            \n",
    "            info['description'] = [d.text for d in what.find('p', class_='prose max-w-full whitespace-pre-line')]\n",
    "\n",
    "            all_info.append(info)\n",
    "\n",
    "\n",
    "        Founders_info.append(all_info)\n",
    "        \n",
    "    # Handle difference in page layouts for Founders.\n",
    "    except:\n",
    "        hold = []\n",
    "        all_info = []\n",
    "\n",
    "        founders = soup.find('div', class_='space-y-4')\n",
    "\n",
    "        for founder in founders.find_all('div', class_='leading-snug'):\n",
    "            \n",
    "            info = {}\n",
    "            name = founder.find('div', class_='font-bold').text\n",
    "            info['name'] = name\n",
    "\n",
    "            divs = [what for what in founder.find_all('div')]\n",
    "\n",
    "            info['role'] = divs[1].text\n",
    "            info['social_media'] = [link['href'] for link in founder.find('div', class_='mt-1 space-x-2').find_all('a')]\n",
    "\n",
    "            all_info.append(info)\n",
    "            hold.append(founder.find('div', class_='font-bold').text)\n",
    "            \n",
    "        Active_founders.append(hold)\n",
    "        Founders_info.append(all_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTION TO SCRAPE ALL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(url): \n",
    "\n",
    "    # Hold the unscraped sites.\n",
    "    unscraped_sites = []\n",
    "\n",
    "    # Pass to BeautifulSoup.\n",
    "    Each_page = requests.get(url)\n",
    "    soup = BeautifulSoup(Each_page.content)\n",
    "\n",
    "    try:\n",
    "        get_companies_info(soup)\n",
    "        get_founders_info(soup)\n",
    "    \n",
    "    except (socket.gaierror, ConnectionError):\n",
    "        unscraped_sites.append(url)\n",
    "\n",
    "    return unscraped_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INITIATE PLACEHOLDER LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_name = []\n",
    "Company_tag = []\n",
    "Short_description = []\n",
    "Founded = []\n",
    "Team_size = []\n",
    "Location = []\n",
    "Country = []\n",
    "Website = [] \n",
    "Active_founders = []\n",
    "Social_media_company = []\n",
    "Founders_info = []\n",
    "Description = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SCRAPE DATA WITHOUT MULTI-THREADING**\n",
    "Scraping the data without multi-threading took approximately `13 minutes`. This is largely dependent on the internect connectivity and can vary based on the connectivity speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "\n",
    "i = 0\n",
    "# Check internet connection.\n",
    "conn = internet_on()\n",
    "\n",
    "if conn is True:\n",
    "\n",
    "    links = get_company_url_and_country()\n",
    "    \n",
    "    while run:\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            unscraped_sites = scrape_info(link)\n",
    "            \n",
    "            print(Company_name[i])\n",
    "            print(Company_tag[i])\n",
    "            print(Short_description[i])\n",
    "            print(Founded[i])\n",
    "            print(Team_size[i])\n",
    "            print(Location[i])\n",
    "            print(Country[i])\n",
    "            print(Website[i])\n",
    "            print(Active_founders[i])\n",
    "            print(Social_media_company[i])\n",
    "            print(Founders_info[i])\n",
    "            print(Description[i])\n",
    "            print('\\n')\n",
    "            i += 1\n",
    "            \n",
    "        # Print unscraped sites.\n",
    "        print(f\"Sites left to scrape are:\", unscraped_sites)\n",
    "\n",
    "        # Check for unscraped sites and reinitiate scraping.\n",
    "        if unscraped_sites != []:\n",
    "            links = unscraped_sites\n",
    "            \n",
    "        else:\n",
    "            print('DONE!')\n",
    "            run = False\n",
    "\n",
    "else:\n",
    "    print(\"Check internet connection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINITIATE PLACEHOLDER LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Company_name = []\n",
    "Company_tag = []\n",
    "Short_description = []\n",
    "Founded = []\n",
    "Team_size = []\n",
    "Location = []\n",
    "Website = [] \n",
    "Active_founders = []\n",
    "Social_media_company = []\n",
    "Founders_info = []\n",
    "Description = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SCRAPE DATA WITH MULTI-THREADING**\n",
    "Scraping the data with multi-threading took approximately `1 minutes,46 seconds`. A significant improvement to the former. This is also largely dependent on the internect connectivity and can vary based on the connectivity speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sites left to scrape are: []\n",
      "DONE!\n",
      "Total runtime: 0:01:20.770924\n"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "unscraped_sites = []\n",
    "i = 0\n",
    "\n",
    "# Check internet connection.\n",
    "conn = internet_on()\n",
    "\n",
    "# Start timer.\n",
    "start_thread = dt.now()\n",
    "\n",
    "if conn is True:\n",
    "\n",
    "    # Get company links.\n",
    "    links = get_company_url_and_country()\n",
    "    \n",
    "    while run:\n",
    "        \n",
    "        with cf.ThreadPoolExecutor() as executor:\n",
    "            executor.map(scrape_info, links)\n",
    "\n",
    "        # Print unscraped sites.\n",
    "        print(f\"Sites left to scrape are:\", unscraped_sites)\n",
    "\n",
    "        # Check for unscraped sites and reinitiate scraping.\n",
    "        if unscraped_sites != []:\n",
    "            links = unscraped_sites\n",
    "            \n",
    "        else:\n",
    "            print('DONE!')\n",
    "            run = False\n",
    "\n",
    "else:\n",
    "    print(\"Check internet connection!\")\n",
    "\n",
    "runtime_thread = (dt.now() - start_thread)\n",
    "print(f'Total runtime: {runtime_thread}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECK LENGTH OF PLACEHOLDER LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Company_name), len(Company_tag), len(Short_description), len(Founded), len(Location), len(Country), len(Team_size), len(Website), len(Social_media_company), len(Founders_info), len(Active_founders), len(Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STORE EXTRACTED DATA IN A DATAFRAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Company_tag</th>\n",
       "      <th>Short_description</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Team_size</th>\n",
       "      <th>Website</th>\n",
       "      <th>Active_founders</th>\n",
       "      <th>Company_social_media</th>\n",
       "      <th>Founders_info</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Airbnb</td>\n",
       "      <td>[W09, Public, Marketplace, Travel]</td>\n",
       "      <td>Book accommodations around the world.</td>\n",
       "      <td>2008</td>\n",
       "      <td>San Francisco, CA, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>5000</td>\n",
       "      <td>http://airbnb.com</td>\n",
       "      <td>[Nathan Blecharczyk, Brian Chesky, Joe Gebbia]</td>\n",
       "      <td>[https://www.linkedin.com/company/airbnb/, htt...</td>\n",
       "      <td>[{'name': 'Nathan Blecharczyk', 'role': 'CTO',...</td>\n",
       "      <td>Founded in August of 2008 and based in San Fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coinbase</td>\n",
       "      <td>[S12, Public]</td>\n",
       "      <td>Buy, sell, and manage cryptocurrencies.</td>\n",
       "      <td>2012</td>\n",
       "      <td>San Francisco, CA, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>500</td>\n",
       "      <td>https://www.coinbase.com</td>\n",
       "      <td>[Brian Armstrong]</td>\n",
       "      <td>[https://twitter.com/coinbase, https://www.fac...</td>\n",
       "      <td>[{'name': 'Brian Armstrong', 'role': 'CEO', 's...</td>\n",
       "      <td>Founded in June of 2012, Coinbase is a digital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DoorDash</td>\n",
       "      <td>[S13, Public, 10]</td>\n",
       "      <td>Restaurant delivery.</td>\n",
       "      <td></td>\n",
       "      <td>San Francisco, CA, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>1600</td>\n",
       "      <td>http://doordash.com</td>\n",
       "      <td>[Andy Fang, Stanley Tang, Tony Xu]</td>\n",
       "      <td>[https://www.linkedin.com/company/doordash/, h...</td>\n",
       "      <td>[{'name': 'Andy Fang', 'role': 'Founder', 'soc...</td>\n",
       "      <td>Founded in 2013, DoorDash is a San Francisco-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GitLab</td>\n",
       "      <td>[W15, Public, DevSecOps, Developer Tools]</td>\n",
       "      <td>A complete DevOps platform delivered as a sing...</td>\n",
       "      <td>2012</td>\n",
       "      <td>San Francisco, CA, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>1200</td>\n",
       "      <td>http://gitlab.com/</td>\n",
       "      <td>[Sid Sijbrandij, Dmitriy Zaporozhets]</td>\n",
       "      <td>[https://www.linkedin.com/company/gitlab-com, ...</td>\n",
       "      <td>[{'name': 'Sid Sijbrandij', 'role': 'CEO', 'so...</td>\n",
       "      <td>GitLab is the first single application for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dropbox</td>\n",
       "      <td>[S07, Public]</td>\n",
       "      <td>Backup and share files in the cloud.</td>\n",
       "      <td>2008</td>\n",
       "      <td>San Francisco, CA, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>4000</td>\n",
       "      <td>http://dropbox.com</td>\n",
       "      <td>[Arash Ferdowsi, Drew Houston]</td>\n",
       "      <td>[https://www.linkedin.com/company/dropbox/, ht...</td>\n",
       "      <td>[{'name': 'Arash Ferdowsi', 'role': '', 'socia...</td>\n",
       "      <td>Dropbox is building the world’s first smart wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company_name                                Company_tag  \\\n",
       "0       Airbnb         [W09, Public, Marketplace, Travel]   \n",
       "1     Coinbase                              [S12, Public]   \n",
       "2     DoorDash                          [S13, Public, 10]   \n",
       "3       GitLab  [W15, Public, DevSecOps, Developer Tools]   \n",
       "4      Dropbox                              [S07, Public]   \n",
       "\n",
       "                                   Short_description Founded  \\\n",
       "0              Book accommodations around the world.    2008   \n",
       "1            Buy, sell, and manage cryptocurrencies.    2012   \n",
       "2                               Restaurant delivery.           \n",
       "3  A complete DevOps platform delivered as a sing...    2012   \n",
       "4               Backup and share files in the cloud.    2008   \n",
       "\n",
       "                 Location Country Team_size                   Website  \\\n",
       "0  San Francisco, CA, USA     USA      5000         http://airbnb.com   \n",
       "1  San Francisco, CA, USA     USA       500  https://www.coinbase.com   \n",
       "2  San Francisco, CA, USA     USA      1600       http://doordash.com   \n",
       "3  San Francisco, CA, USA     USA      1200        http://gitlab.com/   \n",
       "4  San Francisco, CA, USA     USA      4000        http://dropbox.com   \n",
       "\n",
       "                                  Active_founders  \\\n",
       "0  [Nathan Blecharczyk, Brian Chesky, Joe Gebbia]   \n",
       "1                               [Brian Armstrong]   \n",
       "2              [Andy Fang, Stanley Tang, Tony Xu]   \n",
       "3           [Sid Sijbrandij, Dmitriy Zaporozhets]   \n",
       "4                  [Arash Ferdowsi, Drew Houston]   \n",
       "\n",
       "                                Company_social_media  \\\n",
       "0  [https://www.linkedin.com/company/airbnb/, htt...   \n",
       "1  [https://twitter.com/coinbase, https://www.fac...   \n",
       "2  [https://www.linkedin.com/company/doordash/, h...   \n",
       "3  [https://www.linkedin.com/company/gitlab-com, ...   \n",
       "4  [https://www.linkedin.com/company/dropbox/, ht...   \n",
       "\n",
       "                                       Founders_info  \\\n",
       "0  [{'name': 'Nathan Blecharczyk', 'role': 'CTO',...   \n",
       "1  [{'name': 'Brian Armstrong', 'role': 'CEO', 's...   \n",
       "2  [{'name': 'Andy Fang', 'role': 'Founder', 'soc...   \n",
       "3  [{'name': 'Sid Sijbrandij', 'role': 'CEO', 'so...   \n",
       "4  [{'name': 'Arash Ferdowsi', 'role': '', 'socia...   \n",
       "\n",
       "                                         Description  \n",
       "0  Founded in August of 2008 and based in San Fra...  \n",
       "1  Founded in June of 2012, Coinbase is a digital...  \n",
       "2  Founded in 2013, DoorDash is a San Francisco-b...  \n",
       "3  GitLab is the first single application for the...  \n",
       "4  Dropbox is building the world’s first smart wo...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = zip(Company_name, Company_tag, Short_description, Founded, Location, Country, Team_size, Website, Active_founders, Social_media_company, Founders_info, Description)\n",
    "mapped = list(d)\n",
    "df = pd.DataFrame(mapped, columns=['Company_name','Company_tag', 'Short_description', 'Founded', 'Location', 'Country', 'Team_size', 'Website', 'Active_founders', 'Company_social_media', 'Founders_info', 'Description'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPORT DATAFRAME TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ycombinator.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c160159c6242d6cdaae06a36440898c4ef1ee5cfbb9309d174b475858a9b9a1e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('ml_dependencies': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
